{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZQBCHT9ZHy8",
        "outputId": "30dd18c8-ed7f-40d6-8d1e-dc85bd4b3d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import vstack, save_npz\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "# Suppress FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/cs506midterm/train.csv')\n",
        "print(f\"Original number of rows in train data: {len(train_data)}\")\n",
        "\n",
        "# Handling missing values in 'Text' and 'Summary'\n",
        "train_data['Text'].fillna('', inplace=True)\n",
        "train_data['Summary'].fillna('', inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "train_data['review_length'] = train_data['Text'].apply(len)\n",
        "train_data['summary_length'] = train_data['Summary'].apply(len)\n",
        "train_data['helpfulness_ratio'] = train_data['HelpfulnessNumerator'] / (train_data['HelpfulnessDenominator'] + 1)\n",
        "\n",
        "# Save non-text features before filtering out NaN scores\n",
        "non_text_features = train_data[['review_length', 'summary_length', 'helpfulness_ratio']]\n",
        "\n",
        "# Initialize TF-IDF vectorizer and process in chunks\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "chunk_size = 10000\n",
        "text_list = train_data['Text'].tolist()\n",
        "text_features = []\n",
        "\n",
        "# Fit and transform TF-IDF in chunks to avoid memory issues\n",
        "for i in range(0, len(text_list), chunk_size):\n",
        "    chunk = text_list[i:i + chunk_size]\n",
        "    tfidf.fit(chunk)\n",
        "    print(f\"Processed chunk {i // chunk_size + 1} of {len(text_list) // chunk_size + 1}\")\n",
        "\n",
        "    # Transform and save chunk\n",
        "    chunk_features = tfidf.transform(chunk)\n",
        "    text_features.append(chunk_features)\n",
        "\n",
        "# Combine all TF-IDF features into a single sparse matrix\n",
        "text_features = vstack(text_features)\n",
        "print(\"TF-IDF Transformation complete!\")\n",
        "\n",
        "# Save TF-IDF features and model\n",
        "save_npz('/content/drive/MyDrive/cs506midterm/text_features_sparse.npz', text_features)\n",
        "non_text_features.to_csv('/content/drive/MyDrive/cs506midterm/non_text_features.csv', index=False)\n",
        "joblib.dump(tfidf, '/content/drive/MyDrive/cs506midterm/tfidf_model.pkl')\n",
        "\n",
        "print(\"Preprocessing complete! Sparse text features saved to 'text_features_sparse.npz', non-text features to 'non_text_features.csv', and TF-IDF model saved to 'tfidf_model.pkl'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLFAFAQNZUbZ",
        "outputId": "8250f994-6aa9-4ac9-af1c-51048a5395d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of rows in train data: 1697533\n",
            "Processed chunk 1 of 170\n",
            "Processed chunk 2 of 170\n",
            "Processed chunk 3 of 170\n",
            "Processed chunk 4 of 170\n",
            "Processed chunk 5 of 170\n",
            "Processed chunk 6 of 170\n",
            "Processed chunk 7 of 170\n",
            "Processed chunk 8 of 170\n",
            "Processed chunk 9 of 170\n",
            "Processed chunk 10 of 170\n",
            "Processed chunk 11 of 170\n",
            "Processed chunk 12 of 170\n",
            "Processed chunk 13 of 170\n",
            "Processed chunk 14 of 170\n",
            "Processed chunk 15 of 170\n",
            "Processed chunk 16 of 170\n",
            "Processed chunk 17 of 170\n",
            "Processed chunk 18 of 170\n",
            "Processed chunk 19 of 170\n",
            "Processed chunk 20 of 170\n",
            "Processed chunk 21 of 170\n",
            "Processed chunk 22 of 170\n",
            "Processed chunk 23 of 170\n",
            "Processed chunk 24 of 170\n",
            "Processed chunk 25 of 170\n",
            "Processed chunk 26 of 170\n",
            "Processed chunk 27 of 170\n",
            "Processed chunk 28 of 170\n",
            "Processed chunk 29 of 170\n",
            "Processed chunk 30 of 170\n",
            "Processed chunk 31 of 170\n",
            "Processed chunk 32 of 170\n",
            "Processed chunk 33 of 170\n",
            "Processed chunk 34 of 170\n",
            "Processed chunk 35 of 170\n",
            "Processed chunk 36 of 170\n",
            "Processed chunk 37 of 170\n",
            "Processed chunk 38 of 170\n",
            "Processed chunk 39 of 170\n",
            "Processed chunk 40 of 170\n",
            "Processed chunk 41 of 170\n",
            "Processed chunk 42 of 170\n",
            "Processed chunk 43 of 170\n",
            "Processed chunk 44 of 170\n",
            "Processed chunk 45 of 170\n",
            "Processed chunk 46 of 170\n",
            "Processed chunk 47 of 170\n",
            "Processed chunk 48 of 170\n",
            "Processed chunk 49 of 170\n",
            "Processed chunk 50 of 170\n",
            "Processed chunk 51 of 170\n",
            "Processed chunk 52 of 170\n",
            "Processed chunk 53 of 170\n",
            "Processed chunk 54 of 170\n",
            "Processed chunk 55 of 170\n",
            "Processed chunk 56 of 170\n",
            "Processed chunk 57 of 170\n",
            "Processed chunk 58 of 170\n",
            "Processed chunk 59 of 170\n",
            "Processed chunk 60 of 170\n",
            "Processed chunk 61 of 170\n",
            "Processed chunk 62 of 170\n",
            "Processed chunk 63 of 170\n",
            "Processed chunk 64 of 170\n",
            "Processed chunk 65 of 170\n",
            "Processed chunk 66 of 170\n",
            "Processed chunk 67 of 170\n",
            "Processed chunk 68 of 170\n",
            "Processed chunk 69 of 170\n",
            "Processed chunk 70 of 170\n",
            "Processed chunk 71 of 170\n",
            "Processed chunk 72 of 170\n",
            "Processed chunk 73 of 170\n",
            "Processed chunk 74 of 170\n",
            "Processed chunk 75 of 170\n",
            "Processed chunk 76 of 170\n",
            "Processed chunk 77 of 170\n",
            "Processed chunk 78 of 170\n",
            "Processed chunk 79 of 170\n",
            "Processed chunk 80 of 170\n",
            "Processed chunk 81 of 170\n",
            "Processed chunk 82 of 170\n",
            "Processed chunk 83 of 170\n",
            "Processed chunk 84 of 170\n",
            "Processed chunk 85 of 170\n",
            "Processed chunk 86 of 170\n",
            "Processed chunk 87 of 170\n",
            "Processed chunk 88 of 170\n",
            "Processed chunk 89 of 170\n",
            "Processed chunk 90 of 170\n",
            "Processed chunk 91 of 170\n",
            "Processed chunk 92 of 170\n",
            "Processed chunk 93 of 170\n",
            "Processed chunk 94 of 170\n",
            "Processed chunk 95 of 170\n",
            "Processed chunk 96 of 170\n",
            "Processed chunk 97 of 170\n",
            "Processed chunk 98 of 170\n",
            "Processed chunk 99 of 170\n",
            "Processed chunk 100 of 170\n",
            "Processed chunk 101 of 170\n",
            "Processed chunk 102 of 170\n",
            "Processed chunk 103 of 170\n",
            "Processed chunk 104 of 170\n",
            "Processed chunk 105 of 170\n",
            "Processed chunk 106 of 170\n",
            "Processed chunk 107 of 170\n",
            "Processed chunk 108 of 170\n",
            "Processed chunk 109 of 170\n",
            "Processed chunk 110 of 170\n",
            "Processed chunk 111 of 170\n",
            "Processed chunk 112 of 170\n",
            "Processed chunk 113 of 170\n",
            "Processed chunk 114 of 170\n",
            "Processed chunk 115 of 170\n",
            "Processed chunk 116 of 170\n",
            "Processed chunk 117 of 170\n",
            "Processed chunk 118 of 170\n",
            "Processed chunk 119 of 170\n",
            "Processed chunk 120 of 170\n",
            "Processed chunk 121 of 170\n",
            "Processed chunk 122 of 170\n",
            "Processed chunk 123 of 170\n",
            "Processed chunk 124 of 170\n",
            "Processed chunk 125 of 170\n",
            "Processed chunk 126 of 170\n",
            "Processed chunk 127 of 170\n",
            "Processed chunk 128 of 170\n",
            "Processed chunk 129 of 170\n",
            "Processed chunk 130 of 170\n",
            "Processed chunk 131 of 170\n",
            "Processed chunk 132 of 170\n",
            "Processed chunk 133 of 170\n",
            "Processed chunk 134 of 170\n",
            "Processed chunk 135 of 170\n",
            "Processed chunk 136 of 170\n",
            "Processed chunk 137 of 170\n",
            "Processed chunk 138 of 170\n",
            "Processed chunk 139 of 170\n",
            "Processed chunk 140 of 170\n",
            "Processed chunk 141 of 170\n",
            "Processed chunk 142 of 170\n",
            "Processed chunk 143 of 170\n",
            "Processed chunk 144 of 170\n",
            "Processed chunk 145 of 170\n",
            "Processed chunk 146 of 170\n",
            "Processed chunk 147 of 170\n",
            "Processed chunk 148 of 170\n",
            "Processed chunk 149 of 170\n",
            "Processed chunk 150 of 170\n",
            "Processed chunk 151 of 170\n",
            "Processed chunk 152 of 170\n",
            "Processed chunk 153 of 170\n",
            "Processed chunk 154 of 170\n",
            "Processed chunk 155 of 170\n",
            "Processed chunk 156 of 170\n",
            "Processed chunk 157 of 170\n",
            "Processed chunk 158 of 170\n",
            "Processed chunk 159 of 170\n",
            "Processed chunk 160 of 170\n",
            "Processed chunk 161 of 170\n",
            "Processed chunk 162 of 170\n",
            "Processed chunk 163 of 170\n",
            "Processed chunk 164 of 170\n",
            "Processed chunk 165 of 170\n",
            "Processed chunk 166 of 170\n",
            "Processed chunk 167 of 170\n",
            "Processed chunk 168 of 170\n",
            "Processed chunk 169 of 170\n",
            "Processed chunk 170 of 170\n",
            "TF-IDF Transformation complete!\n",
            "Preprocessing complete! Sparse text features saved to 'text_features_sparse.npz', non-text features to 'non_text_features.csv', and TF-IDF model saved to 'tfidf_model.pkl'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import load_npz, hstack\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Load the sparse TF-IDF features and non-text features\n",
        "text_features = load_npz('/content/drive/MyDrive/cs506midterm/text_features_sparse.npz')\n",
        "non_text_features = pd.read_csv('/content/drive/MyDrive/cs506midterm/non_text_features.csv')\n",
        "\n",
        "# Load and clean target variable\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/cs506midterm/train.csv')\n",
        "train_data = train_data.dropna(subset=['Score']).reset_index(drop=True)  # Remove rows with NaN in 'Score'\n",
        "y = train_data['Score']\n",
        "\n",
        "# Filter non-text features and text features to match the length of cleaned target data\n",
        "non_text_features = non_text_features.loc[train_data.index].reset_index(drop=True)\n",
        "text_features = text_features[train_data.index, :]\n",
        "\n",
        "# Concatenate the features\n",
        "X = hstack([text_features, non_text_features])\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_pred_rf)}\")\n",
        "print(classification_report(y_val, y_pred_rf))\n",
        "\n",
        "# Support Vector Machine Classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_val)\n",
        "print(\"Support Vector Machine Classifier:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_pred_svm)}\")\n",
        "print(classification_report(y_val, y_pred_svm))\n",
        "\n",
        "# Save the best model\n",
        "joblib.dump(rf_model, '/content/drive/MyDrive/cs506midterm/best_model.pkl')\n",
        "print(\"Best model saved as 'best_model.pkl'.\")\n"
      ],
      "metadata": {
        "id": "Cd1heaRYZado"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.sparse import load_npz, hstack\n",
        "import joblib\n",
        "\n",
        "# Load the test data and TF-IDF model\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/cs506midterm/test.csv')\n",
        "tfidf = joblib.load('/content/tfidf_model.pkl')\n",
        "\n",
        "# Apply feature engineering to the test set\n",
        "test_data['Text'].fillna('', inplace=True)\n",
        "test_data['review_length'] = test_data['Text'].apply(len)\n",
        "test_data['summary_length'] = test_data['Summary'].apply(len)\n",
        "test_data['helpfulness_ratio'] = test_data['HelpfulnessNumerator'] / (test_data['HelpfulnessDenominator'] + 1)\n",
        "\n",
        "# Transform text data using the TF-IDF model\n",
        "text_features_test = tfidf.transform(test_data['Text'])\n",
        "non_text_features_test = test_data[['review_length', 'summary_length', 'helpfulness_ratio']]\n",
        "\n",
        "# Combine sparse text features and non-text features\n",
        "X_test = hstack([text_features_test, non_text_features_test])\n",
        "\n",
        "# Load the trained model and predict\n",
        "best_model = joblib.load('best_model.pkl')\n",
        "test_predictions = best_model.predict(X_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({'Id': test_data['Id'], 'Score': test_predictions})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file 'submission.csv' created.\")"
      ],
      "metadata": {
        "id": "78e9K9P2Ze9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('submission.csv')"
      ],
      "metadata": {
        "id": "LswVyWYVZkVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}